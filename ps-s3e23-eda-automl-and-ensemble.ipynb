{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/valerybonneau/ps-s3e23-eda-automl-and-ensemble?scriptVersionId=193453875\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Table of content\n\n0. [Introduction](#zero)\n1. [Import Libraries and Data](#one)\n2. [EDA](#two)\n3. [Auto Ml test](#three)\n4. [Data Preparation](#four)\n5. [Random Forest](#five)\n6. [Logistic Regression](#six)\n7. [XGBoost](#seven)\n8. [CatBoost](#eight)\n9. [LightGBM](#nine)\n10. [ExtraTreesClassifier](#ten)\n11. [HistGradientBoostingClassifier](#eleven)\n12. [Neural Network - Scikit and Keras](#twelve)\n13. [Ensemble implementation](#thirteen)\n14. [Conclusion and Submission](#fourteen)","metadata":{}},{"cell_type":"markdown","source":"# 0. Introduction\nThis is my first public notebook.\n- After a quick EDA, I'll try Autogluon to give me a base score.\n- Then I'll run Random Forest to check features importances.\n- Finally I'll try different algorithm based on autogluon results (and ideas I could get while discussing).\n\nI'll try to reach top 20% but the main goal is to improve my skills and knowledge.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":false,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-10-19T08:14:28.212599Z","iopub.execute_input":"2023-10-19T08:14:28.213352Z","iopub.status.idle":"2023-10-19T08:14:28.22682Z","shell.execute_reply.started":"2023-10-19T08:14:28.213315Z","shell.execute_reply":"2023-10-19T08:14:28.225547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"one\"></a>\n# 1. Import Libraries and Data","metadata":{}},{"cell_type":"code","source":"# Math and data manipulation packages\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import randint, uniform\n\n# DataViz packages\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Preprocessing packages\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import RepeatedStratifiedKFold, StratifiedKFold\n\nfrom sklearn.preprocessing import PowerTransformer, FunctionTransformer, PolynomialFeatures\n\n# Model packages\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nfrom xgboost import XGBClassifier, XGBRegressor\nimport lightgbm as lgb\nfrom catboost import CatBoostClassifier, Pool\n\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import HistGradientBoostingClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.linear_model import Perceptron\n\n# Neural Network \nimport tensorflow as tf\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\n# Ensemble model build\nfrom sklearn.ensemble import VotingClassifier\n\n# Metrics and optimization\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.metrics import roc_auc_score, accuracy_score, f1_score\n\n# Execution tim\nimport time\n\nsns.set()\n# make sure we can see needed columns and rows\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\n\nnp.set_printoptions(linewidth=195, edgeitems=5)\n\nforce_row_wise=True","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-10-19T08:14:34.280296Z","iopub.execute_input":"2023-10-19T08:14:34.280632Z","iopub.status.idle":"2023-10-19T08:14:39.490411Z","shell.execute_reply.started":"2023-10-19T08:14:34.280596Z","shell.execute_reply":"2023-10-19T08:14:39.489238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/playground-series-s3e23/train.csv')\ntest = pd.read_csv('/kaggle/input/playground-series-s3e23/test.csv')\ntrain.head(10)","metadata":{"execution":{"iopub.status.busy":"2023-10-19T08:14:45.941912Z","iopub.execute_input":"2023-10-19T08:14:45.942724Z","iopub.status.idle":"2023-10-19T08:14:46.501308Z","shell.execute_reply.started":"2023-10-19T08:14:45.942675Z","shell.execute_reply":"2023-10-19T08:14:46.500253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.describe().T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## First Observation\nEvery feature has a relevant type. There is not NaN values. I'll study the data further during the EDA.","metadata":{}},{"cell_type":"markdown","source":"<a id='two'></a>\n# 2. EDA","metadata":{}},{"cell_type":"code","source":"binary = train.drop('id', axis=1)\nbinary['defects'] = binary['defects'].map({True:1, False:0})\n\nfeatures = binary.columns.tolist()\nfeatures = features[:-1]","metadata":{"execution":{"iopub.status.busy":"2023-10-19T08:14:50.842964Z","iopub.execute_input":"2023-10-19T08:14:50.843315Z","iopub.status.idle":"2023-10-19T08:14:50.862335Z","shell.execute_reply.started":"2023-10-19T08:14:50.843286Z","shell.execute_reply":"2023-10-19T08:14:50.861258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Distribution","metadata":{}},{"cell_type":"code","source":"# adapted from https://www.kaggle.com/code/ambrosm/pss3e23-eda-which-makes-sense\n_, axs = plt.subplots(7, 3, figsize=(12,14))\nfor col, ax in zip(binary.columns, axs.ravel()):\n    if binary[col].dtype == float:\n        ax.hist(binary[col], bins=100, color='red')\n    else:\n        no_float = binary[col].value_counts()\n        ax.bar(no_float.index, no_float, color='red')\n    ax.set_title(col)\nplt.tight_layout\nplt.subplots_adjust(hspace=0.5)  # Increase vertical space between subplots\nplt.suptitle('Features distributions')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**First Observations**<br>\n- All features are right skeed.\n- `lOComment` and `locCodeAndComment` are 71.6% and 91.9% made of zeros\n- I will apply Log Transformation","metadata":{}},{"cell_type":"code","source":"# adapted from https://www.kaggle.com/code/ambrosm/pss3e23-eda-which-makes-sense\n_, axs = plt.subplots(7, 3, figsize=(12,14))\nfor col, ax in zip(binary.columns, axs.ravel()):\n    ax.hist(np.log1p(binary[col]), bins=100, color='green')\n    ax.set_title(col)\nplt.tight_layout\nplt.subplots_adjust(hspace=0.5)  # Increase vertical space between subplots\nplt.suptitle('Features distributions after log transformation')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Correlation Matrix","metadata":{}},{"cell_type":"markdown","source":"The features seems very correlated in general, with values as high as 1, or 0.9, 0.8 in many cases.\nStill, there is no feature highly correlated to the target (max is 0.3 or -.3).<br>\n**Also, as it is a classification problem, the correlation is not as important as it would be in a regression problem.**<br>\nRemoving feature that don't bring a lot of values could be interesting to decrease the preocessing time though. \nFor the time being, I can keep all features and I will decide after running a first Random Forest to show feature importances.","metadata":{}},{"cell_type":"code","source":"sns.set(rc={'figure.figsize':(20,10)})\ncorr = binary.corr()\nmask = np.triu(np.ones_like(corr, dtype=bool))\ncmap = sns.diverging_palette(100, 7, s = 75, l = 40, n = 20, center = 'light', as_cmap = True)\n\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=1, center=0, square=True, linewidths=.5, cbar_kws={'shrink':.5}, annot=True, fmt='.1f')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is no categorical feature.\nAnd right now, I don't see how to create categories from the different features.","metadata":{}},{"cell_type":"markdown","source":"## Class Distribution\nThe target class is umbalanced (roughly 8:2). After going through reading and other notebooks, I will not use SMOTE or other techniques. I'll use class_weight='balanced' when the model offers it.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10,5))\nsns.countplot(binary, y=\"defects\");","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(binary['defects'].value_counts(normalize=True)*100).round(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='three'></a>\n# 3. Auto ML test\nI'll run autogluon and submit a first prediction based on the result.<br>","metadata":{}},{"cell_type":"code","source":"!pip install autogluon > nul","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from autogluon.tabular import TabularDataset, TabularPredictor\nautoml = TabularPredictor(\n    problem_type='binary',\n    label='defects',\n    eval_metric='roc_auc'\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"automl.fit(train, presets='best_quality')\nautoml.leaderboard()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = automl.predict(test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = predictions.map({False: 0, True: 1})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The result is 0.66175 wich is pretty disappointing**<br>\nI'll focus on RanfomForest and see if I can identify important features.","metadata":{}},{"cell_type":"markdown","source":"<a id='four'></a>\n# 4. Data Preparation","metadata":{}},{"cell_type":"markdown","source":"**Creation of X_train and y_train**","metadata":{}},{"cell_type":"code","source":"X_train = binary.drop('defects', axis=1)\ny_train = binary['defects']\nX_test = test.drop('id', axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-10-19T08:15:02.938681Z","iopub.execute_input":"2023-10-19T08:15:02.939045Z","iopub.status.idle":"2023-10-19T08:15:02.958296Z","shell.execute_reply.started":"2023-10-19T08:15:02.939019Z","shell.execute_reply":"2023-10-19T08:15:02.957066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Transformed version**","metadata":{}},{"cell_type":"code","source":"transformer = FunctionTransformer(np.log1p)\n\nX_train_t = transformer.fit_transform(X_train)\nX_test_t = transformer.transform(X_test)\n\nX_train_t = pd.DataFrame(data=X_train_t, columns=X_train.columns)\nX_test_t = pd.DataFrame(data=X_test_t, columns=X_test.columns)","metadata":{"execution":{"iopub.status.busy":"2023-10-19T08:15:05.406757Z","iopub.execute_input":"2023-10-19T08:15:05.407126Z","iopub.status.idle":"2023-10-19T08:15:05.483061Z","shell.execute_reply.started":"2023-10-19T08:15:05.407099Z","shell.execute_reply":"2023-10-19T08:15:05.481859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Cross-validation strategy","metadata":{}},{"cell_type":"code","source":"# adapted from https://www.kaggle.com/code/ambrosm/pss3e23-eda-which-makes-sense\n# Thanks a lot to AmbrosM (https://www.kaggle.com/ambrosm)\n\ndef cross_val(model, X_train_t, y_train):\n    tic = time.time()\n       \n    kf = StratifiedKFold(shuffle=True, random_state=73)\n    \n    roc_auc = []\n    for fold, (indX_tr, indX_va) in enumerate(kf.split(X_train, y_train)):\n        X_tr = X_train_t.iloc[indX_tr]\n        X_va = X_train_t.iloc[indX_va]\n        y_tr = y_train.iloc[indX_tr]\n        y_va = y_train.iloc[indX_va]\n        model.fit(X_tr, y_tr)\n        if hasattr(model, \"predict_proba\"):\n            y_va_pred = model.predict_proba(X_va)[:, 1]\n        else:\n            y_va_pred = model.predict(X_va)\n\n        roc_auc.append(roc_auc_score(y_va, y_va_pred))\n    roc_auc = np.array(roc_auc).mean()\n    \n    tac= time.time()\n    print(f'execution time of {model}: {round((tac-tic),2)} seconds')\n    return roc_auc","metadata":{"execution":{"iopub.status.busy":"2023-10-19T08:15:11.636865Z","iopub.execute_input":"2023-10-19T08:15:11.637254Z","iopub.status.idle":"2023-10-19T08:15:11.646082Z","shell.execute_reply.started":"2023-10-19T08:15:11.637224Z","shell.execute_reply":"2023-10-19T08:15:11.644653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# adapted from https://www.kaggle.com/code/ambrosm/pss3e23-eda-which-makes-sense\n\nscores = []\nscore = cross_val(make_pipeline(\n    StandardScaler(),\n    PolynomialFeatures(2, include_bias=False),\n    LogisticRegression(C=0.01, solver='newton-cholesky', penalty='l2', max_iter=1000, class_weight=None)),\n    X_train_t, y_train)\nscores.append(('LogisticRegression', score))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Searching for best parameters\nI have used loops and ploted the results using that function.\nI did it manually becasue I want to understand and see the evolution for each parameter.","metadata":{}},{"cell_type":"code","source":"def plot_score(scores):\n    sns.scatterplot(x=[x for x, y in scores], \n                   y=[y for x,y in scores])\n    plt.xscale('linear')\n    plt.show()\n    print(scores)\n    return True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='five'></a>\n# 5. Random Forest Testing\nAs the submission file must contains probabilities and not binary values, RandomForestRegressor seems better than RandomForestClassifier.<br>\nI'll test both models anyhow as RandomForestClassifier should work fine with the training set.","metadata":{}},{"cell_type":"code","source":"score = cross_val(\n    RandomForestClassifier(bootstrap=True, \n                           max_depth=9, \n                           min_samples_leaf=200, \n                           min_samples_split=7,\n                           n_estimators=70,\n                           max_features=1.0,\n                           class_weight='balanced_subsample',\n                           random_state=73),\n        X_train_t, y_train)\nscores.append(('RandomForestClassifier', score))  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for randomforestregressor if needed later\nscorerf= []\nfor n_estimators in [100, 200, 300, 400, 500]:\n    score = cross_val(\n        RandomForestRegressor(n_estimators=300,\n                              max_features=1.0,\n                              max_depth=8,\n                              min_samples_leaf=72,\n                              min_samples_split=16,\n                              bootstrap=True,\n                              random_state=73,\n                              n_jobs=-1),\n            X_train_t, y_train)\n    scorerf.append(('RandomForestRegressor', score))  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Both models come with the same conclusion that `loc` is the most importante features and by far.<br> Regarding the other features, there importance varies but stays very low in any case. I'll keep them all anyhow for the time being.","metadata":{}},{"cell_type":"markdown","source":"<a id='six'></a>\n# 6. Logistic Regression","metadata":{}},{"cell_type":"code","source":"score = cross_val(make_pipeline(\n    StandardScaler(),\n    PolynomialFeatures(2, include_bias=False),\n    LogisticRegression(C=0.001, solver='newton-cholesky', penalty='l2', max_iter=1000, class_weight='balanced')),\n    X_train_t, y_train)\nscores.append(('LogisticRegression', score))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='seven'></a>\n# 7. XGBoost","metadata":{}},{"cell_type":"code","source":"!pip install xgboost","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = {\n    'max_depth': 6,\n    'subsample': 0.6,\n    'colsample_bytree': 0.7,\n    'learning_rate': 0.02,\n    'n_estimators': 800,\n    'tree_method': 'hist',\n    'random_state': 73,\n}\n\nscore = cross_val(\n    XGBClassifier(\n        objective='binary:logistic',\n        eval_metric='auc',\n        **params),\n        X_train_t, y_train)\nscores.append(('XGBClassifier0', score))","metadata":{"execution":{"iopub.status.busy":"2023-10-19T11:21:34.965841Z","iopub.execute_input":"2023-10-19T11:21:34.966202Z","iopub.status.idle":"2023-10-19T11:22:17.208199Z","shell.execute_reply.started":"2023-10-19T11:21:34.966174Z","shell.execute_reply":"2023-10-19T11:22:17.207126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# adapted from https://www.kaggle.com/code/adaubas/pss3e23-comparison-xgboost-parameters#.79065-on-public-LB\nparams = {\n    'tree_method'        : 'hist',\n    'random_state'       : 73,\n    'max_depth'          : 4,   \n    'colsample_bytree'   : 1.,  \n    'colsample_bynode'   : .8, \n    'subsample'          : .7,\n    'min_child_weight'   : 30, \n    'learning_rate'      : .02, \n    \n    'n_estimators'       : 800, # Fixed by using early stopping with another seed\n}\n\nscore =  cross_val(\n    make_pipeline(\n        ColumnTransformer(\n            # Drop redondant columns : detected in documentation and by using permutation importance\n            transformers = [('select', 'drop', [\"d\", \"e\", \"branchCount\", 'iv(g)', 't', 'b', 'n', 'lOCode', 'v', 'i'])], \n            remainder = 'passthrough'),\n        XGBClassifier(**params)), \n    X_train_t, y_train)\nscores.append(('XGBClassifier1', score))","metadata":{"execution":{"iopub.status.busy":"2023-10-19T11:29:40.134651Z","iopub.execute_input":"2023-10-19T11:29:40.135047Z","iopub.status.idle":"2023-10-19T11:30:05.38592Z","shell.execute_reply.started":"2023-10-19T11:29:40.134991Z","shell.execute_reply":"2023-10-19T11:30:05.384835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score = cross_val(\n    XGBClassifier(\n        objective='binary:logistic',\n        eval_metric='auc',\n        booster='gbtree',\n        learning_rate=0.05,\n        gamma=0.75,\n        max_depth=5,\n        n_estimators=110,\n        random_state=73),\n        X_train_t, y_train)\nscores.append(('XGBClassifier', score))  ","metadata":{"execution":{"iopub.status.busy":"2023-10-19T11:15:17.197721Z","iopub.execute_input":"2023-10-19T11:15:17.198356Z","iopub.status.idle":"2023-10-19T11:16:13.822451Z","shell.execute_reply.started":"2023-10-19T11:15:17.198305Z","shell.execute_reply":"2023-10-19T11:16:13.821338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### XGboostRegressor","metadata":{}},{"cell_type":"code","source":"score = cross_val(\n    XGBRegressor(\n        objective='binary:logistic',\n        eval_metric='auc',\n        colsample_bytree=0.9,\n        gamma=0,\n        learning_rate=0.1,\n        max_depth=3,\n        min_child_weight=2,\n        n_estimators=200,\n        reg_alpha=0.01,\n        reg_lambda=10,\n        subsample=0.9,\n        random_state=73),\n        X_train_t, y_train)\nscores.append(('XGBRegressor', score))  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgbr = XGBRegressor(objective='binary:logistic',\n                    eval_metric='auc',\n                    colsample_bytree=0.9,\n                    gamma=0,\n                    learning_rate=0.1,\n                    max_depth=3,\n                    min_child_weight=2,\n                    n_estimators=200,\n                    reg_alpha=0.01,\n                    reg_lambda=10,\n                    subsample=0.9)\nxgbr.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='eight'></a>\n# 8. CatBoost","metadata":{}},{"cell_type":"code","source":"!pip install catboost","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"{'random_strength': 0.5,\n 'learning_rate': 0.03,\n 'l2_leaf_reg': 9,\n 'iterations': 500,\n 'grow_policy': 'SymmetricTree',\n 'depth': 6,\n 'border_count': 64,\n 'bagging_temperature': 0.5}","metadata":{}},{"cell_type":"code","source":"score = cross_val(\n    CatBoostClassifier(\n        loss_function='CrossEntropy',\n        learning_rate=0.05,\n        iterations=500,\n        depth=5,\n        grow_policy='SymmetricTree',\n        l2_leaf_reg=12,\n        border_count=62,\n        random_strength=1,\n        random_seed=73,\n        verbose=0),\n        X_train_t, y_train)\nscores.append(('CatBoostClassifier', score))  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='nine'></a>\n# 9. LightGBM","metadata":{}},{"cell_type":"code","source":"!pip install lightgbm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dart model","metadata":{}},{"cell_type":"code","source":"parameters = {\n    'objective': 'binary',\n    'metric': 'auc',\n    'is_unbalance': 'true',\n    'boosting': 'dart',\n    'num_leaves': 23,\n    'feature_fraction': 0.5,\n    'bagging_fraction': 0.85,\n    'bagging_freq': 20,\n    'learning_rate': 0.1,\n    'verbose': 0,\n    'random_state':73\n}\nscore = cross_val(lgb.LGBMClassifier(**parameters), X_train_t, y_train)\nscores.append(('LGBMClassifier', score))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='ten'></a>\n# 10. ExtraTreesClassifier","metadata":{}},{"cell_type":"code","source":"score = cross_val(\n    ExtraTreesClassifier(\n        max_depth=100,\n        max_features=1.0,\n        min_samples_leaf=100,\n        n_estimators=250,\n        min_samples_split=5,\n        bootstrap=False,\n        random_state=73,\n        verbose=0),\n        X_train_t, y_train)\nscores.append(('ExtraTreesClassifier', score))  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='eleven'></a>\n# 11. HistGradientBoostingClassifier","metadata":{}},{"cell_type":"code","source":"score = cross_val(\n    HistGradientBoostingClassifier(\n        max_depth=20,\n        max_iter=500,\n        learning_rate=0.0125,\n        max_leaf_nodes=30,\n        l2_regularization=0.1,\n        random_state=73,\n        verbose=0),\n        X_train_t, y_train)\nscores.append(('HistGradientBoostingClassifier', score))  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='thirteen'></a>\n# 13. Ensemble","metadata":{}},{"cell_type":"code","source":"best_params_rfc = {'bootstrap': True, 'max_depth': 9, 'min_samples_leaf': 200, 'min_samples_split': 7, 'n_estimators': 70}\nrfc = RandomForestClassifier(bootstrap=True,                            \n                             max_depth=9,\n                             min_samples_leaf=200, \n                             min_samples_split=7,\n                             n_estimators=70,\n                             max_features=1.0,\n                             class_weight='balanced_subsample',\n                             random_state=73)","metadata":{"execution":{"iopub.status.busy":"2023-10-19T08:15:33.943214Z","iopub.execute_input":"2023-10-19T08:15:33.943748Z","iopub.status.idle":"2023-10-19T08:15:33.949708Z","shell.execute_reply.started":"2023-10-19T08:15:33.943705Z","shell.execute_reply":"2023-10-19T08:15:33.948427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rfr = RandomForestRegressor(n_estimators=300,\n                              max_features=1.0,\n                              max_depth=8,\n                              min_samples_leaf=72,\n                              min_samples_split=16,\n                              bootstrap=True,\n                              random_state=73,\n                              n_jobs=-1)","metadata":{"execution":{"iopub.status.busy":"2023-10-19T08:15:36.262277Z","iopub.execute_input":"2023-10-19T08:15:36.262626Z","iopub.status.idle":"2023-10-19T08:15:36.268561Z","shell.execute_reply.started":"2023-10-19T08:15:36.262599Z","shell.execute_reply":"2023-10-19T08:15:36.266967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_params_lr = {'solver': 'liblinear', 'penalty': 'l1', 'max_iter': 1000, 'C': 0.01}\nlr = LogisticRegression(**best_params_lr,                      \n                      class_weight='balanced',\n                      n_jobs=-1,\n                      random_state=12345)\n\nlr = make_pipeline(\n        StandardScaler(),\n        PolynomialFeatures(2, include_bias=False),\n        LogisticRegression(C=0.001, solver='newton-cholesky', penalty='l2', max_iter=1000, class_weight='balanced')\n)","metadata":{"execution":{"iopub.status.busy":"2023-10-19T08:15:41.433838Z","iopub.execute_input":"2023-10-19T08:15:41.434232Z","iopub.status.idle":"2023-10-19T08:15:41.441046Z","shell.execute_reply.started":"2023-10-19T08:15:41.434202Z","shell.execute_reply":"2023-10-19T08:15:41.439714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_params_xgbc = {'reg_lambda': 0.1, 'reg_alpha': 51.2, 'n_estimators': 130, 'max_depth': 7, 'learning_rate': 0.2, 'gamma': 0}\nxgbc0 = XGBClassifier(objective='binary:logistic',\n            eval_metric='auc',\n            booster='gbtree',\n            learning_rate=0.05,\n            gamma=0.75,\n            max_depth=5,\n            n_estimators=110,\n            random_state=73)","metadata":{"execution":{"iopub.status.busy":"2023-10-19T11:32:59.086037Z","iopub.execute_input":"2023-10-19T11:32:59.086383Z","iopub.status.idle":"2023-10-19T11:32:59.091495Z","shell.execute_reply.started":"2023-10-19T11:32:59.086356Z","shell.execute_reply":"2023-10-19T11:32:59.090759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = {\n    'max_depth': 6,\n    'subsample': 0.6,\n    'colsample_bytree': 0.7,\n    'learning_rate': 0.02,\n    'n_estimators': 800,\n    'tree_method': 'hist',\n    'random_state': 73,\n}\n\nxgbc1 = XGBClassifier(objective='binary:logistic',\n                      eval_metric='auc',\n                      **params)","metadata":{"execution":{"iopub.status.busy":"2023-10-19T11:32:56.886934Z","iopub.execute_input":"2023-10-19T11:32:56.887285Z","iopub.status.idle":"2023-10-19T11:32:56.893109Z","shell.execute_reply.started":"2023-10-19T11:32:56.887258Z","shell.execute_reply":"2023-10-19T11:32:56.891947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# adapted from https://www.kaggle.com/code/adaubas/pss3e23-comparison-xgboost-parameters#.79065-on-public-LB\nparams = {\n    'tree_method'        : 'hist',\n    'random_state'       : 73,\n    'max_depth'          : 4,   # I want only weak learners\n    'colsample_bytree'   : 1.,  # To be sure that 'loc' will be in all trees\n    'colsample_bynode'   : .8, \n    'subsample'          : .7,\n    'min_child_weight'   : 30, \n    'learning_rate'      : .02, # fixed to early stop before 1000 estimatores\n    \n    'n_estimators'       : 800, # Fixed by using early stopping with another seed\n}\n\nxgbc2 = make_pipeline(\n        ColumnTransformer(\n            # Drop redondant columns : detected in documentation and by using permutation importance\n            transformers = [('select', 'drop', [\"d\", \"e\", \"branchCount\", 'iv(g)', 't', 'b', 'n', 'lOCode', 'v', 'i'])], \n            remainder = 'passthrough'),\n        XGBClassifier(**params))","metadata":{"execution":{"iopub.status.busy":"2023-10-19T11:33:56.213529Z","iopub.execute_input":"2023-10-19T11:33:56.213939Z","iopub.status.idle":"2023-10-19T11:33:56.221239Z","shell.execute_reply.started":"2023-10-19T11:33:56.21391Z","shell.execute_reply":"2023-10-19T11:33:56.219951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_params_xgbr = {}\nxgbr = XGBRegressor(objective='binary:logistic',\n                    eval_metric='auc',\n                    colsample_bytree=0.9,\n                    gamma=0,\n                    learning_rate=0.1,\n                    max_depth=3,\n                    min_child_weight=2,\n                    n_estimators=200,\n                    reg_alpha=0.01,\n                    reg_lambda=10,\n                    subsample=0.9)\n\nxgbr._estimator_type = \"classifier\"","metadata":{"execution":{"iopub.status.busy":"2023-10-19T08:15:46.971416Z","iopub.execute_input":"2023-10-19T08:15:46.971789Z","iopub.status.idle":"2023-10-19T08:15:46.978068Z","shell.execute_reply.started":"2023-10-19T08:15:46.971763Z","shell.execute_reply":"2023-10-19T08:15:46.976777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_params_cbr = {'random_strength': 0.5, 'learning_rate': 0.03, 'l2_leaf_reg': 9, 'iterations': 500,\n                   'grow_policy': 'SymmetricTree', 'depth': 6, 'border_count': 64, 'bagging_temperature': 0.5}\n\ncb = CatBoostClassifier(\n            loss_function='CrossEntropy',\n            learning_rate=0.05,\n            iterations=500,\n            depth=5,\n            grow_policy='SymmetricTree',\n            l2_leaf_reg=12,\n            border_count=62,\n            random_strength=1,\n            random_seed=73,\n            verbose=0)","metadata":{"execution":{"iopub.status.busy":"2023-10-19T08:15:50.333798Z","iopub.execute_input":"2023-10-19T08:15:50.334145Z","iopub.status.idle":"2023-10-19T08:15:50.340246Z","shell.execute_reply.started":"2023-10-19T08:15:50.334118Z","shell.execute_reply":"2023-10-19T08:15:50.338784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# adapted from https://www.kaggle.com/code/iqmansingh/software-defect-ensemble-lgbm-sampling#Optuna-Tuning-LGBM\n\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\nparam0 = {'n_estimators': 957, 'max_depth': 5, 'learning_rate': 0.014544218759128154, \n          'min_child_weight': 2.7336861099989402, 'min_child_samples': 44, \n          'subsample': 0.6786668835159727, 'subsample_freq': 2, 'colsample_bytree': 0.7487125800084695}\n\nparam1 = {'n_estimators': 844, 'max_depth': 5, 'learning_rate': 0.011533926188770152, \n          'min_child_weight': 0.7353926016580375, 'min_child_samples': 22, \n          'subsample': 0.7440626280651244, 'subsample_freq': 3, 'colsample_bytree': 0.5193554106905941}\n\nparam2 = {'n_estimators': 1374, 'max_depth': 8, 'learning_rate': 0.0030464211231350236, \n          'min_child_weight': 2.6773708665039124, 'min_child_samples': 36, \n          'subsample': 0.5495916550024219, 'subsample_freq': 1, 'colsample_bytree': 0.5065855075580986}\n\nlgbm0 =    make_pipeline(\n        ColumnTransformer(\n            # Drop redondant columns : detected in documentation and by using permutation importance\n            transformers = [('select', 'drop', [\"d\", \"e\", \"branchCount\", 'iv(g)', 't', 'b', 'n', 'lOCode', 'v', 'i'])], \n            remainder = 'passthrough'),\n        lgb.LGBMClassifier(**param0))\n    \nlgbm1 =    make_pipeline(\n        ColumnTransformer(\n            # Drop redondant columns : detected in documentation and by using permutation importance\n            transformers = [('select', 'drop', [\"d\", \"e\", \"branchCount\", 'iv(g)', 't', 'b', 'n', 'lOCode', 'v', 'i'])], \n            remainder = 'passthrough'),\n        lgb.LGBMClassifier(**param1))\n    \nlgbm2 =    make_pipeline(\n        ColumnTransformer(\n            # Drop redondant columns : detected in documentation and by using permutation importance\n            transformers = [('select', 'drop', [\"d\", \"e\", \"branchCount\", 'iv(g)', 't', 'b', 'n', 'lOCode', 'v', 'i'])], \n            remainder = 'passthrough'),\n        lgb.LGBMClassifier(**param2))","metadata":{"execution":{"iopub.status.busy":"2023-10-19T12:26:14.905167Z","iopub.execute_input":"2023-10-19T12:26:14.905546Z","iopub.status.idle":"2023-10-19T12:26:14.915693Z","shell.execute_reply.started":"2023-10-19T12:26:14.905516Z","shell.execute_reply":"2023-10-19T12:26:14.914354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"etc = ExtraTreesClassifier(\n            max_depth=100,\n            max_features=1.0,\n            min_samples_leaf=100,\n            n_estimators=300,\n            min_samples_split=5,\n            bootstrap=False,\n            random_state=73,\n            verbose=0)","metadata":{"execution":{"iopub.status.busy":"2023-10-19T08:15:57.722Z","iopub.execute_input":"2023-10-19T08:15:57.722382Z","iopub.status.idle":"2023-10-19T08:15:57.727737Z","shell.execute_reply.started":"2023-10-19T08:15:57.722345Z","shell.execute_reply":"2023-10-19T08:15:57.726511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_params_hgbc = {'l2_regularization': 0.1,\n 'learning_rate': 0.1,\n 'max_depth': 5,\n 'max_iter': 469,\n 'max_leaf_nodes': 90}\nhgbc = HistGradientBoostingClassifier(\n            max_depth=20,\n            max_iter=500,\n            learning_rate=0.0125,\n            max_leaf_nodes=30,\n            l2_regularization=0.1,\n            random_state=73,\n            verbose=0)","metadata":{"execution":{"iopub.status.busy":"2023-10-19T08:15:59.643455Z","iopub.execute_input":"2023-10-19T08:15:59.643972Z","iopub.status.idle":"2023-10-19T08:15:59.650087Z","shell.execute_reply.started":"2023-10-19T08:15:59.643939Z","shell.execute_reply":"2023-10-19T08:15:59.64917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I searched the best weights distribution by looping through different values. Not optimal though.","metadata":{}},{"cell_type":"code","source":"rfr._estimator_type = \"classifier\"\nestimators1 =(\n    [('lr', lr),\n     ('rf', rfc),\n     ('xgbc0', xgbc0),\n     ('xgbc1', xgbc1),\n     ('xgbc2', xgbc2),\n     ('cbc', cb),\n     ('hgb', hgbc),\n    ('lgbm0', lgbm0),\n    ('lgbm1', lgbm1),\n    ('lgbm2', lgbm2)])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vcl1 = VotingClassifier(estimators=estimators1,\n                        weights=[0.25, 0.5, 1.5, 3.5, 2.75, 0.75, 0.5, 3.75, 2.75, 1.75],\n                        voting='soft',\n                        n_jobs=-1)\nvcl1.fit(X_train_t, y_train)\npredictions = vcl1.predict_proba(X_test_t)[:,1]","metadata":{"execution":{"iopub.status.busy":"2023-10-19T14:12:10.970087Z","iopub.execute_input":"2023-10-19T14:12:10.970443Z","iopub.status.idle":"2023-10-19T14:14:31.310884Z","shell.execute_reply.started":"2023-10-19T14:12:10.970417Z","shell.execute_reply":"2023-10-19T14:14:31.309844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='forteen'></a>\n# 14. Conclusion and Submission","metadata":{}},{"cell_type":"markdown","source":"My final LB score is : 0.79035. I'm still in the top20%, which was one of my goals but we'll see how it goes with the final score.\nAdding a propre neural network could improve the results but at that stage, I'll keep it like this.","metadata":{}},{"cell_type":"code","source":"output_sample = pd.read_csv('/kaggle/input/playground-series-s3e23/sample_submission.csv')\noutput = pd.DataFrame({'id': output_sample.id, 'defects': predictions})\noutput.to_csv('submission.csv', index=False, sep=',')\nprint(\"Your submission was successfully saved!\")","metadata":{"execution":{"iopub.status.busy":"2023-10-19T14:14:53.536231Z","iopub.execute_input":"2023-10-19T14:14:53.536561Z","iopub.status.idle":"2023-10-19T14:14:53.734191Z","shell.execute_reply.started":"2023-10-19T14:14:53.536535Z","shell.execute_reply":"2023-10-19T14:14:53.733435Z"},"trusted":true},"execution_count":null,"outputs":[]}]}